{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNRnSXqH7vJTLPGQT0wyCQp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhnguyen17/JAPAN2020/blob/main/FinalCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZjAIbA0semE"
      },
      "source": [
        "# Few shot Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax5Qb5B6n7iA"
      },
      "source": [
        "#Tweet/Short text dataset\n",
        "\n",
        "\n",
        "tweet = [(\"Bitcoin\",\"Sign up with this website to receive TEN DOLLARS in Bitcoin! First time users only\"),\n",
        "         (\"Marvel\",\"Marvel Mobile, play the official Spider-man game on ur mobile web right now.\"),\n",
        "         (\"Police\",\"Exclusive: Watch how George Floyed was killed in police custody on this link\"),\n",
        "         (\"iPhone\",\"Visit our website for a BIG DISCOUNT on your iPhone purchase!\"),\n",
        "         (\"Walmart\",\"Lucky shoppers! You've won a THOUSAND DOLLAR Walmart gift card! Go to this link to claim now.\"),\n",
        "         (\"soccer\", \"World Cup 2022 is here! Check out the game schedule on this link RIGHT NOW...\"), \n",
        "         (\"travel\", \"2 hours left to join our site. Your complimentary FOUR STAR Ibiza Holiday or ONE THOUSAND cash await collection\"),                       \n",
        "         (\"dating\",\"Talk sexy!! Fall in love in the worlds most discreet text dating service. Sign up at this link and see who you could meet.\")\n",
        "         \n",
        "         #(\"Spanish\", \"Learn Spanish with natives. Log onto this link now\"),\n",
        "         #(\"Records\",\"Congratulations ur awarded either a year supply of CDs from Virgin Records when subscribing to our channel\"),\n",
        "         #(\"donations\",\"Urgent!!!! Please we still need your help! Donations are still needed for Alicia surgery.. \"),\n",
        "         #(\"tennis\",\"Get free tennis lessons by clicking on this link!\"),\n",
        "         ]\n",
        "\n",
        "test_tweet = [(\"Spanish\", \"Learn Spanish with natives. Log onto this link now\"),\n",
        "         (\"Records\",\"Congratulations ur awarded either a year supply of CDs from Virgin Records when subscribing to our channel\"),\n",
        "         (\"donations\",\"Urgent Alicia needs surgery now!! Donations accepted on this website.. \"),\n",
        "         (\"tennis\",\"Get free tennis lessons by clicking on this link!\")]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVYlLM87jmpj"
      },
      "source": [
        "#Save / Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmMLYErOjrjY"
      },
      "source": [
        "#@title Mount drive and Save model method { display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def save():\n",
        "  path = \"/content/gdrive/MyDrive/Colab Notebooks/T5_models\" \n",
        "  sys.path.append(os.path.abspath(path))\n",
        "\n",
        "  tokenizer.save_pretrained(path)\n",
        "  t5_model.save_pretrained(path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qunwJ0LgsmEv"
      },
      "source": [
        "# T5 Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jK4yY4HrDs6"
      },
      "source": [
        "#@title Install and import all needed packages { display-mode: \"form\" }\n",
        "###Installing transformer\n",
        "print('Install transformers')\n",
        "\n",
        "!pip install transformers==2.9.0\n",
        "\n",
        "###Import all needed package\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDtC8irnrbfC"
      },
      "source": [
        "#@title Get methods (model, tokenizer, optimizer) { display-mode: \"form\" }\n",
        "#eliminate randomness for reproduction purposes\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "def get_model(model_name):\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "  t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "  return tokenizer, t5_model\n",
        "def get_optimizer(model, epsilon = 1e-8, learning_rate = 1e-4 ):\n",
        "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "  optimizer_grouped_parameters = [\n",
        "      {\n",
        "          \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "          \"weight_decay\": 0.0,\n",
        "      },\n",
        "      {\n",
        "          \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "          \"weight_decay\": 0.0,\n",
        "      },\n",
        "  ]\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4, eps=1e-8)\n",
        "  return optimizer\n",
        "def graph_training_loss(loss_data):\n",
        "  steps = [i for i in range(len(loss_data))]\n",
        "  plt.plot(steps, loss_data)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZiqiFZLsqhT"
      },
      "source": [
        "#@title Train and decoder methods { display-mode: \"form\" }\n",
        "def train(data, tokenizer, t5_model,epochs = 30):\n",
        "  optimizer = get_optimizer(t5_model)\n",
        "  loss_data = []\n",
        "  for epoch in range(epochs):\n",
        "    print (\"epoch \",epoch)\n",
        "    running_loss = 0\n",
        "    for input,output in data:\n",
        "      input_kw = \"\btweet: \"+input+ \" </s>\"\n",
        "      output_tweet = output+\" </s>\"\n",
        "\n",
        "      tokenized_input = tokenizer.encode_plus(input_kw,  max_length=100, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "      tokenized_output = tokenizer.encode_plus(output_tweet, max_length=100, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "\n",
        "      # the forward function that creates the correct decoder_input_ids\n",
        "      output = t5_model(input_ids=tokenized_input[\"input_ids\"], \n",
        "                        lm_labels=tokenized_output[\"input_ids\"],\n",
        "                        decoder_attention_mask=tokenized_output[\"attention_mask\"],\n",
        "                        attention_mask=tokenized_input[\"attention_mask\"])\n",
        "      loss = output[0]\n",
        "      logits = output[1]\n",
        "      running_loss += loss.item() \n",
        "\n",
        "      #calculate gradient\n",
        "      loss.backward()\n",
        "\n",
        "      #update the parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      #clear gradients of all Var\n",
        "      optimizer.zero_grad()\n",
        "    loss_data.append(running_loss)\n",
        "    print(running_loss)\n",
        "\n",
        "  graph_training_loss(loss_data)\n",
        "\n",
        "  return (tokenizer,t5_model)\n",
        "\n",
        "def beam_decoder(keyword):\n",
        "\n",
        "  test_kw = 'tweet: ' + keyword  + '</s>'\n",
        "  test_tokenized = tokenizer.encode_plus(test_kw, return_tensors=\"pt\")\n",
        "  beam_outputs = t5_model.generate(\n",
        "    input_ids = test_tokenized[\"input_ids\"], \n",
        "    attention_mask=test_tokenized[\"attention_mask\"],\n",
        "    max_length=64, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=2, \n",
        "    early_stopping=True)\n",
        "\n",
        "  # now we have 2 output sequences\n",
        "  print(\"Output for Beam decoder:\\n\" + 100 * '-')\n",
        "  for i, beam_output in enumerate(beam_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
        "\n",
        "def sample_decoder(keyword):\n",
        "  test_kw = 'tweet: ' + keyword +'</s>'\n",
        "  test_tokenized = tokenizer.encode_plus(test_kw, return_tensors=\"pt\")\n",
        "\n",
        "  # use temperature to decrease the sensitivity to low probability candidates\n",
        "  sample_output = t5_model.generate(\n",
        "      input_ids = test_tokenized[\"input_ids\"], \n",
        "      attention_mask=test_tokenized[\"attention_mask\"],\n",
        "      do_sample=True, \n",
        "      max_length=64, \n",
        "      top_k=0, \n",
        "      temperature=0.7\n",
        "  )\n",
        "\n",
        "  print(\"Output for Sample Decoder:\\n\" + 100 * '-')\n",
        "  print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
        "  \n",
        "def topPtopK_decoder(keyword):\n",
        "  test_kw = 'tweet: ' + keyword +'</s>'\n",
        "  test_tokenized = tokenizer.encode_plus(test_kw, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "  sample_outputs = t5_model.generate(\n",
        "    input_ids = test_tokenized[\"input_ids\"], \n",
        "    attention_mask=test_tokenized[\"attention_mask\"],\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=2)\n",
        "\n",
        "  print(\"Output for Top P top K decoder:\\n\" + 100 * '-')\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2S06q9DdDMU"
      },
      "source": [
        "###Code to train tweet dataset\n",
        "\n",
        "tokenizer , t5_model = get_model('t5-base')\n",
        "\n",
        "###Replace tweet data\n",
        "tokenizer, t5_model = train(tweet, tokenizer, t5_model,epochs = 20)\n",
        "\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv8gLV08toi1"
      },
      "source": [
        "#Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_iNstGtcuGBo"
      },
      "source": [
        "#@title Print output \n",
        "#!pip install datasets\n",
        "\n",
        "#!pip install git+https://github.com/google-research/bleurt.git\n",
        "\n",
        "#from datasets import load_metric\n",
        "#metric = load_metric(\"bleurt\")\n",
        "def print_output(func):\n",
        "  reference = []\n",
        "\n",
        "  for input,output in test_tweet:\n",
        "    reference.append(output)\n",
        "    print(\"Keyword: \"+ input)\n",
        "    func(input)\n",
        "\n",
        "def print_output(keyword):\n",
        "  print(\"Keyword: \"+ keyword)\n",
        "  beam_decoder(keyword)\n",
        "  print()\n",
        "  sample_decoder(keyword)\n",
        "  print()\n",
        "  topPtopK_decoder(keyword)\n",
        "  print()\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Huht_7QYyR5"
      },
      "source": [
        "#@title \bReload Model { display-mode: \"code\" }\n",
        "path = \"/content/gdrive/MyDrive/Colab Notebooks/T5_models\" \n",
        "sys.path.append(os.path.abspath(path))\n",
        "\n",
        "tokenizer.from_pretrained(path)\n",
        "t5_model.from_pretrained(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D93RjTmmOqZW"
      },
      "source": [
        "print_output('bitcoin')\n",
        "print()\n",
        "print_output('tennis')\n",
        "print()\n",
        "print_output('vacation')\n",
        "print()\n",
        "print_output('machine learning')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}