{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO0+Ij1J2el866IVfdKtMNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhnguyen17/JAPAN2020/blob/main/FinalCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZjAIbA0semE"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax5Qb5B6n7iA"
      },
      "source": [
        "#Tweet/Short text dataset\n",
        "\n",
        "\n",
        "tweet = [(\"Bitcoin\",\"Sign up with this website to receive TEN DOLLARS in Bitcoin! First time users only\"),\n",
        "         (\"Marvel\",\"Marvel Mobile, play the official Spider-man game on ur mobile web right now.\"),\n",
        "         (\"Police\",\"Exclusive: Watch how George Floyed was killed in police custody on this link\"),\n",
        "         (\"iPhone\",\"Visit our website for a BIG DISCOUNT on your iPhone purchase!\"),\n",
        "         (\"Walmart\",\"Lucky shoppers! You've won a THOUSAND DOLLAR Walmart gift card! Go to this link to claim now.\"),\n",
        "         (\"soccer\", \"World Cup 2022 is here! Check out the game schedule on this link RIGHT NOW...\"), \n",
        "         (\"travel\", \"2 hours left to join our site. Your complimentary FOUR STAR Ibiza Holiday or ONE THOUSAND cash await collection\"),                       \n",
        "         (\"dating\",\"Talk sexy!! Fall in love in the worlds most discreet text dating service. Sign up at this link and see who you could meet.\")\n",
        "         \n",
        "         #(\"Spanish\", \"Learn Spanish with natives. Log onto this link now\"),\n",
        "         #(\"Records\",\"Congratulations ur awarded either a year supply of CDs from Virgin Records when subscribing to our channel\"),\n",
        "         #(\"donations\",\"Urgent!!!! Please we still need your help! Donations are still needed for Alicia surgery.. \"),\n",
        "         #(\"tennis\",\"Get free tennis lessons by clicking on this link!\"),\n",
        "         ]\n",
        "\n",
        "test_tweet = [(\"Spanish\", \"Learn Spanish with natives. Log onto this link now\"),\n",
        "         (\"Records\",\"Congratulations ur awarded either a year supply of CDs from Virgin Records when subscribing to our channel\"),\n",
        "         (\"donations\",\"Urgent Alicia needs surgery now!! Donations accepted on this website.. \"),\n",
        "         (\"tennis\",\"Get free tennis lessons by clicking on this link!\")]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVYlLM87jmpj"
      },
      "source": [
        "#Save / Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmMLYErOjrjY",
        "outputId": "2344b5b6-0fef-4a7d-8cfd-f47ee3a10e60"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def save():\n",
        "  path = \"/content/gdrive/MyDrive/Colab Notebooks/T5_models\" \n",
        "  sys.path.append(os.path.abspath(path))\n",
        "\n",
        "  tokenizer.save_pretrained(path)\n",
        "  t5_model.save_pretrained(path)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qunwJ0LgsmEv"
      },
      "source": [
        "# T5 Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jK4yY4HrDs6",
        "outputId": "d1e9ab13-2e48-4353-95d3-ce2b982d90de"
      },
      "source": [
        "\n",
        "###Installing transformer\n",
        "print('Install transformers')\n",
        "\n",
        "!pip install transformers==2.9.0\n",
        "\n",
        "###Import all needed package\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Install transformers\n",
            "Collecting transformers==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.19.5)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDtC8irnrbfC"
      },
      "source": [
        "\n",
        "#eliminate randomness for reproduction purposes\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "def get_model(model_name):\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "  t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "  return tokenizer, t5_model\n",
        "def get_optimizer(model, epsilon = 1e-8, learning_rate = 1e-4 ):\n",
        "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "  optimizer_grouped_parameters = [\n",
        "      {\n",
        "          \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "          \"weight_decay\": 0.0,\n",
        "      },\n",
        "      {\n",
        "          \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "          \"weight_decay\": 0.0,\n",
        "      },\n",
        "  ]\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4, eps=1e-8)\n",
        "  return optimizer\n",
        "def graph_training_loss(loss_data):\n",
        "  steps = [i for i in range(len(loss_data))]\n",
        "  plt.plot(steps, loss_data)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZiqiFZLsqhT"
      },
      "source": [
        "\n",
        "def train(data, tokenizer, t5_model,epochs = 30):\n",
        "  optimizer = get_optimizer(t5_model)\n",
        "  loss_data = []\n",
        "  for epoch in range(epochs):\n",
        "    print (\"epoch \",epoch)\n",
        "    running_loss = 0\n",
        "    for input,output in data:\n",
        "      input_kw = \"\btweet: \"+input+ \" </s>\"\n",
        "      output_tweet = output+\" </s>\"\n",
        "\n",
        "      tokenized_input = tokenizer.encode_plus(input_kw,  max_length=100, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "      tokenized_output = tokenizer.encode_plus(output_tweet, max_length=100, pad_to_max_length=True,return_tensors=\"pt\")\n",
        "\n",
        "      # the forward function that creates the correct decoder_input_ids\n",
        "      output = t5_model(input_ids=tokenized_input[\"input_ids\"], \n",
        "                        lm_labels=tokenized_output[\"input_ids\"],\n",
        "                        decoder_attention_mask=tokenized_output[\"attention_mask\"],\n",
        "                        attention_mask=tokenized_input[\"attention_mask\"])\n",
        "      loss = output[0]\n",
        "      logits = output[1]\n",
        "      running_loss += loss.item() \n",
        "\n",
        "      #calculate gradient\n",
        "      loss.backward()\n",
        "\n",
        "      #update the parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      #clear gradients of all Var\n",
        "      optimizer.zero_grad()\n",
        "    loss_data.append(running_loss)\n",
        "    print(running_loss)\n",
        "\n",
        "  graph_training_loss(loss_data)\n",
        "\n",
        "  return (tokenizer,t5_model)\n",
        "\n",
        "def beam_decoder(keyword, t5_model, tokenizer):\n",
        "\n",
        "  test_kw = 'tweet: ' + keyword  + '</s>'\n",
        "  test_tokenized = tokenizer.encode_plus(test_kw, return_tensors=\"pt\")\n",
        "  beam_outputs = t5_model.generate(\n",
        "    input_ids = test_tokenized[\"input_ids\"], \n",
        "    attention_mask=test_tokenized[\"attention_mask\"],\n",
        "    max_length=64, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=1, \n",
        "    early_stopping=True)\n",
        "\n",
        "  print(\"Output for Beam decoder:\\n\" + 100 * '-')\n",
        "  # now we have 2 output sequences\n",
        "  for i, sample_output in enumerate(beam_outputs):\n",
        "    print(tokenizer.decode(sample_output, skip_special_tokens=True))\n",
        "\n",
        "def sample_decoder(keyword , t5_model, tokenizer):\n",
        "  test_kw = 'tweet: ' + keyword +'</s>'\n",
        "  test_tokenized = tokenizer.encode_plus(test_kw, return_tensors=\"pt\")\n",
        "\n",
        "  # use temperature to decrease the sensitivity to low probability candidates\n",
        "  sample_output = t5_model.generate(\n",
        "      input_ids = test_tokenized[\"input_ids\"], \n",
        "      attention_mask=test_tokenized[\"attention_mask\"],\n",
        "      do_sample=True, \n",
        "      max_length=64, \n",
        "      top_k=0, \n",
        "      temperature=0.7\n",
        "  )\n",
        "\n",
        "  print(\"Output for Sample Decoder:\\n\" + 100 * '-')\n",
        "  print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
        "  \n",
        "  \n",
        "def topPtopK_decoder(keyword , t5_model, tokenizer):\n",
        "  res = []\n",
        "  test_kw = 'tweet: ' + keyword +'</s>'\n",
        "  test_tokenized = tokenizer.encode_plus(test_kw, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "  sample_outputs = t5_model.generate(\n",
        "    input_ids = test_tokenized[\"input_ids\"], \n",
        "    attention_mask=test_tokenized[\"attention_mask\"],\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3)\n",
        "\n",
        "  print(\"Output for Top P top K decoder:\\n\" + 100 * '-')\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "    print(tokenizer.decode(sample_output, skip_special_tokens=True))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2S06q9DdDMU"
      },
      "source": [
        "###Code to train tweet dataset\n",
        "\n",
        "tokenizer , t5_model = get_model('t5-base')\n",
        "\n",
        "###Replace tweet data\n",
        "tokenizer, t5_model = train(tweet, tokenizer, t5_model,epochs = 20)\n",
        "\n",
        "save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv8gLV08toi1"
      },
      "source": [
        "#Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iNstGtcuGBo"
      },
      "source": [
        "\n",
        "def print_output(func , model ,tokenizer):\n",
        "  reference = []\n",
        "\n",
        "  for input,output in test_tweet:\n",
        "    reference.append(output)\n",
        "    print(\"Keyword: \"+ input)\n",
        "    func(input, model ,tokenizer)\n",
        "\n",
        "def print_output(keyword, model ,tokenizer):\n",
        "  print(\"Keyword: \"+ keyword)\n",
        "  beam_decoder(keyword, model ,tokenizer)\n",
        "  print()\n",
        "  sample_decoder(keyword, model ,tokenizer)\n",
        "  print()\n",
        "  topPtopK_decoder(keyword, model ,tokenizer)\n",
        "  print()\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Huht_7QYyR5"
      },
      "source": [
        "path = \"/content/gdrive/MyDrive/Colab Notebooks/T5_models\" \n",
        "sys.path.append(os.path.abspath(path))\n",
        "tokenizer= T5Tokenizer.from_pretrained(path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D93RjTmmOqZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d95fb29-7d7c-41d5-aa09-f3ac4a7aaf62"
      },
      "source": [
        "#Filled in your desired keywords for message generating\n",
        "print_output('showbiz', t5_model, tokenizer)\n",
        "print()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyword: showbiz\n",
            "Output for Beam decoder:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Exclusive: showbizbiz.com has a new look at what's going on in the worlds of television and the arts - and this is the first time we've seen it!\n",
            "\n",
            "Output for Sample Decoder:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Showbizbizbizbiznews - Join the conversation! Join the conversation!<extra_id_0>bizbiz.com. This entry was posted in showbiz, showbiz news, showbiz news, showbiz culture, showbiz + 1 other person to discuss this\n",
            "\n",
            "Output for Top P top K decoder:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sign up with this website to receive TEN DOLLARS in the low 48 hours of sunday, september 20. Also check out our other blog posts: showbizbiz, get your own account and start creating memories on this\n",
            "Showbizbizbiza is on the fritz! Follow the bar at 10/9c on twitter @blogger_and_drohkopf & follow the bizbizbizbizbiz on facebook!\n",
            "SENDBITE! Showbizbizbiz & britney spears: see what's going on in the worlds of video games, TV, and the supernatural! SPEAKBITE: IM SPIRIT\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}